# parser chain and topic map configuration
chain.file=configs/parser/chains/squid_only_chain.json
chain.topic.map.file=configs/parser/topic_maps/squid_only_map.json

# set the flink job name
flink.job.name=edu.parse

# kafka broker connection settings
kafka.bootstrap.servers=localhost.localdomain:9092
kafka.client.id=edu.parse
kafka.group.id=edu.parse
kafka.auto.offset.reset=earliest
kafka.acks=all


topic.output=edu.triage.input
topic.error=edu.parser.error

# schema registry connection settings
schema.registry.url=http://localhost.localdomain:7788/api/v1

# enable collection of original source and set directory to write parquet files
original.enabled=true
original.basepath=hdfs:///user/training/cybersec/data/original-source

signature.enabled=false
key.private.file=private_key.der

# Flink job settings
# number of instances that Flink will run
parallelism=1
checkpoint.interval.ms=60000

# Secure SSL and kerberos settings
#schema.registry.client.ssl.trustStorePath=${SCHEMA_REGISTRY_TRUSTSTORE}
#schema.registry.client.ssl.trustStorePassword=${SCHEMA_REGISTRY_TRUSTSTORE_PASSWORD}

#kafka.security.protocol=SASL_SSL
#kafka.sasl.mechanism=PLAIN
#kafka.ssl.truststore.location=${KAFKA_TRUSTSTORE}
#kafka.ssl.truststore.password=${KAFKA_TRUSTSTORE_PASSWORD}
#kafka.sasl.jaas.config=org.apache.kafka.common.security.plain.PlainLoginModule required username="${CYBERSEC_USER}" password="${CYBERSEC_PASSWORD}";